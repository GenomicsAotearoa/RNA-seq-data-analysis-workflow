[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "RNA-seq data analysis workflow",
    "section": "",
    "text": "This document aims to get you started with analysing your own RNA-seq data by providing an example workflow. At each of the major steps in the workflow we will examine issues that could confound your analysis and consider how different available methods address these issues.\nRather than prescribing what methods you should use, this document aims to make the user aware of the biological and statistical challenges involved in analysing RNA-seq data and asks how different methods address those challenges. By the end of this document you should be familiar with the general workflow of RNA-seq analysis and be ready to ask informed questions about the specific methods you should select going forward.\nThis document is written in the form of a workshop which will run over two mornings from 9am - 1pm. The workshop includes a small example dataset which will be used by the workshop Instructor to demonstrate an analysis. Attendees may choose to follow along with the analysis in real time or they may choose to focus on note-taking. The workshop includes short exercises to highlight key messages and reinforce your understanding.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#getting-started-with-rna-seq-data-analysis",
    "href": "index.html#getting-started-with-rna-seq-data-analysis",
    "title": "RNA-seq data analysis workflow",
    "section": "",
    "text": "This document aims to get you started with analysing your own RNA-seq data by providing an example workflow. At each of the major steps in the workflow we will examine issues that could confound your analysis and consider how different available methods address these issues.\nRather than prescribing what methods you should use, this document aims to make the user aware of the biological and statistical challenges involved in analysing RNA-seq data and asks how different methods address those challenges. By the end of this document you should be familiar with the general workflow of RNA-seq analysis and be ready to ask informed questions about the specific methods you should select going forward.\nThis document is written in the form of a workshop which will run over two mornings from 9am - 1pm. The workshop includes a small example dataset which will be used by the workshop Instructor to demonstrate an analysis. Attendees may choose to follow along with the analysis in real time or they may choose to focus on note-taking. The workshop includes short exercises to highlight key messages and reinforce your understanding.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#workshop-timeline",
    "href": "index.html#workshop-timeline",
    "title": "RNA-seq data analysis workflow",
    "section": "Workshop timeline",
    "text": "Workshop timeline\nThis workshop runs over two mornings, from 9am - 1pm. We will take one 15 minute break at approximately 10:45, and shorter breaks in between.\n\nDay 1\n\nAn overview of RNA-seq data analysis workflows\n\n\nQuality assessment\n\n\nAdaptor trimming and filtering\n\n\nMapping and counting methods\n(this section will be an overview of techniques and approaches, and will not be executed in depth)\n\n\nExploratory analysis\n\n\n\nDay 2\n\nConcepts for identifying differentially expressed genes\n\n\nLimma for differentially expressed genes\n\n\nDESeq2 for differentially expressed genes\n\n\nOver-representation analysis",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#a-note-on-software",
    "href": "index.html#a-note-on-software",
    "title": "RNA-seq data analysis workflow",
    "section": "A note on software",
    "text": "A note on software\nDuring the workshop, code will be run in real-time to demonstrate the analysis. This will be run on the NeSI OpenOnDemand platform which already has all the required software and R packages loaded.\nWhere you are working will dictate what you need to install and how you will need to go about it, therefore this document does not include instructions to install software. We will include notes about loading R packages, and\n\nAttribution notice\nThis workshop is an overhauled version on an existing version of RNA-seq data analysis by Genomics Aotearoa and NeSI. It incorporates, and takes inspiration from, various training materials produced by the Harvard Bioinformatics Core: visit their github and from The Carpentries, specifically their RNA-seq analysis with Bioconductor workshop.\n\n Next Page",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "episode_3.html",
    "href": "episode_3.html",
    "title": "Quantifying Gene Expression",
    "section": "",
    "text": "In order to draw conclusions about gene expression from our reads, we need to first identify which region of the genome reads originated from and then quantify the number of reads from that region. There are many tools available to carry out quantification, and we will not cover the arguments for and against each tool. We encourage you to carry out a literature search, and discuss with experienced individuals in your field, before committing to a particular quantification procedure. Here we will give a brief overview of two approaches with considerably different underlying methodology, give an example tool for each, and then demonstrate one tool for the purpose of progressing to the next stage of our workflow.\nThe first approach to read quantification involves aligning reads to the genome and then counting the number of reads that are aligned to certain genomic features (e.g., exons).\nThe second approach we will introduce involves creating ‘pseudocounts’. In this method, reads are not aligned to the genome. Instead, they are pseudoaligned to kmers within the transcriptome and their abundance is estimated.\nIn today’s workflow we will demonstrate genome alignment and counting, and will provide examples of other workflows that use pseudocounting and abundance estimation.",
    "crumbs": [
      "Day One",
      "Quantifying Gene Expression"
    ]
  },
  {
    "objectID": "episode_3.html#alignment-with-hisat2",
    "href": "episode_3.html#alignment-with-hisat2",
    "title": "Quantifying Gene Expression",
    "section": "Alignment with HISAT2",
    "text": "Alignment with HISAT2\n\nPreparation of the genome\nWhen carrying out alignment, the first requirement is a genome which has been indexed. Indexing is a process to organise the genome so that our alignment algorithms can match reads to the genome easily, without having to scan the entire genome.\nNavigate to the Genome directory and view the contents. You should see two files, a .gtf and a .fa file. We will use the .gtf file towards the end of this episode.\ncd ~/RNA_seq/Genome\n\nls \nThe .gtf file (General Transfer Format) contains information about the genome, such as gene names etc., all stored in a one-line-per-feature format, while the .fa (FASTA) file contains sequence.\nWe will now use hisat2 to index the genome using the hisat2-build command. We will specify the number of threads (-p 4), and specify the input file (-f for FASTA file type).\n# index file:\nhisat2-build -p 4 -f Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa Saccharomyces_cerevisiae.R64-1-1.dna.toplevel\n\nls\nWe can now see many new .ht2 files, which make up the indexed version of the genome.\n\n\nAlignment on the genome\nNow that we have an indexed genome, we can align our sequences. To do this we will need to know:\n\nWhere the sequence information is stored (e.g., fastq files).\nWhat kind of sequencing file we have (e.g., Single end or Paired end).\nWhere the indexes and genome are stored.\nWhere the mapping files will be stored.\n\nOnce we have that information we are ready to align our sequences. First, navigate to the RNA_seq directory, and run hisat2 on the first sample. We will use the -x flag to specify the basename of the index, the -U flag to specify the comma-separated list of files containing unpaired reads to be aligned, and the -S flag to write SAM alignment output files.\ncd ~/RNA_seq\n\nhisat2 -x Genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U Raw/SRR014335-chr1.fastq -S SRR014335.sam\nOutput should look something like this:\n125090 reads; of these:\n\n  125090 (100.00%) were unpaired; of these:\n\n    15108 (12.08%) aligned 0 times\n\n    88991 (71.14%) aligned exactly 1 time\n\n    20991 (16.78%) aligned &gt;1 times\n\n87.92% overall alignment rate\nNow that we have confirmed this code works for our first sample, we will use a loop to align the rest of the samples. We will make a new directory called Mapping, which we will use to store all of our mapping output files. Then navigate to the Trimmed directory and execute the for loop to align all samples.\nmkdir Mapping\n\ncd ~/RNA_seq/Trimmed\n\nfor filename in *\ndo\nbase=$(basename ${filename} .trimmed.fastq)\nhisat2 -p 4 -x ../Genome/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel -U $filename -S ../Mapping/${base}.sam --summary-file ../Mapping/${base}_summary.txt\ndone\nCheck the output directory (Mapping) and see the different files that have been produced. You should see both .sam and _summary.txt files for each sample.\nLet’s look at the SAM file format.\nless SRR014335-chr1.sam\nThe file begins with an optional header which is used to include human-readable metadata such as the source of the data, reference sequence, method of alignment etc.,. Following the header is the alignment section. Each line contains information corresponding to the alignment of a single read. Each alignment line has 11 mandatory fields for essential mapping information and a variable number of other fields for aligner-specific information.\n\n\nConverting SAM to BAM\nSAM files are tab-delimited text files containing information for each individual read and it’s alignment to the genome. SAM files are large, so the first thing we do with them is convert them to BAM files: compressed, binary versions of the file which reduce time and can themselves be indexed for efficiency when we interact with them.\nWe will convert the SAM file to the BAM format using the samtools program with the view command. Use the -S flag to specify the input is a SAM file, and the -b flag to specify BAM as the output.\nNote that you will see a warning “fail to read the header from…”, this is normal.\nfor filename in *.sam\ndo\nbase=$(basename ${filename} .sam)\nsamtools view -S -b ${filename} -o ${base}.bam\ndone\n\nls\nNext we will sort the BAM files, which will mean our future steps are more efficient. From the samtools program we will use the sort command, with the -o flag to specify where the output file should go.\nfor filename in *.bam\ndo\nbase=$(basename ${filename} .bam)\nsamtools sort -o ${base}_sorted.bam ${filename}\ndone\n\n\nMapping statistics\nUsing the sorted BAM file, you can now easily calculate some basic mapping statistics using the samtools flagstat command.\nsamtools flagstat SRR014335-chr1_sorted.bam\nBasic statistics shown by flagstat will be slightly different from those in the summary file generated by HISAT2 due to different “totals” that are used for comparisons. flagstat compares the number of alignments while HISAT2 compares the number of reads mapped. This is because reads can be mapped/aligned to more than one reference location, and these reads have a “primary” and “secondary” alignment (see section 1.2 of the SAM specifications). For example, the percent overall alignment in the HISAT2 summary will be equivalent to the percent primary mapped evaluated by flagstat. To get the number of reads that aligned 0 times (summary file), the equivalent statistic from flagstat would be subtracting the number of mapped reads from the number of total alignments.\n\n\nMultiQC\nHISAT2 output data can be incorporated into the MultiQC report. Copy the summary files generated by HISAT2 into the MultiQC directory and re-run multiqc.\nNavigate to the updated report and view it in your browser.\ncd ~/RNA_seq/MultiQC\ncp ../Mapping/*summary* ./\nmultiqc .\n\n\n\nMultiQC after alignment\n\n\nYou might notice that the report labels the summary files as “Bowtie2”. This is because the outputs are identical to those generated by Bowtie2, and MultiQC recognises them as Bowtie2.",
    "crumbs": [
      "Day One",
      "Quantifying Gene Expression"
    ]
  },
  {
    "objectID": "episode_3.html#read-summarisation",
    "href": "episode_3.html#read-summarisation",
    "title": "Quantifying Gene Expression",
    "section": "Read summarisation",
    "text": "Read summarisation\nSequencing reads often need to be assigned to genomic features of interest after they are mapped to the reference genome. This process is often called read summarisation or read quantification. Read summarisation is required by a number of downstream analyses such as gene expression analysis and histone modification analysis. The output of read summarisation is a count table, in which the number of reads assigned to each feature in each library is recorded. In our case each feature is an exon. To carry out counting we will use the featureCounts tool from the Subread package.\nNavigate to the RNA_seq directory, create a new directory called Counts and cd into that directory. From there run the featureCounts command which will require five different flags. We will use the -a flag to specify the annotation file, in this case the .gtf file we noticed earlier. The -o flag names the output file (which we will call yeast_counts.txt), while the -T flag specifies the number of threads/CPUs used for mapping. The -t flag is used to control the feature type in the GTF annotation file, we will use exon (which is also the default). Finally, the -g flag specifies the attribute type in the GTF file. We will use the gene_id option (again, the default).\ncd ~/RNA_seq\n\nmkdir Counts\n\ncd Counts\n\nfeatureCounts -a ../Genome/Saccharomyces_cerevisiae.R64-1-1.99.gtf -o ./yeast_counts.txt -T 2 -t exon -g gene_id ../Mapping/*sorted.bam\nUpdate the MultiQC report by copying over the output data in Counts.\ncd ~/RNA_seq/MultiQC\n\ncp ../Counts/* .\n\nmultiqc .\nWe have now generated counts! We can now take these through to the next stage of our analysis: preparing to identify differentially expressed genes.\n\n Previous Page   Next Page",
    "crumbs": [
      "Day One",
      "Quantifying Gene Expression"
    ]
  },
  {
    "objectID": "episode_1.html",
    "href": "episode_1.html",
    "title": "The RNA-seq workflow",
    "section": "",
    "text": "What are the key steps in an RNA-seq data analysis workflow?\nWhat are the peculiar features of RNA-seq data, and how do we address these features to make sure our analysis is unbiased and accurate?\nWhat methods are available to handle RNA-seq data analysis?",
    "crumbs": [
      "Day One"
    ]
  },
  {
    "objectID": "episode_1.html#the-rna-seq-workflow-overview",
    "href": "episode_1.html#the-rna-seq-workflow-overview",
    "title": "The RNA-seq workflow",
    "section": "The RNA-seq workflow overview",
    "text": "The RNA-seq workflow overview\nWe will assume that the aim of our RNA-seq experiment is to identify a set of genes which are differentially expressed between two or more groups, and to derive biologically meaningful information from our gene list(s).\nAssuming this is the case, there are many methods that can be applied to this type of analysis and this can be overwhelming. Here we will look at what we call the core workflow - the major steps that you will take to go from the raw FASTA files (the output from an RNA-seq experiment) to an informative list of differentially expressed genes.\n\n(replace with a figure):\nQuality assessment of the raw data\nAdapter removal and cleanup\nQuantification of gene expression\nIdentifying differentially expressed genes\nOver-representation analysis\nAs you will see, for some steps there are multiple tools that take a functionally similar approach (e.g., quality assessment and adapter removal). For other steps, especially for quantification and identification of differentially expressed genes, there are different methodologies available. These methodologies take different routes to address complex biological, computational and statistical questions. Let’s look at some peculiar features of RNA-seq data and identify the challenges these raise for us as data analysts.",
    "crumbs": [
      "Day One"
    ]
  },
  {
    "objectID": "episode_1.html#what-are-the-main-challenges-or-decision-points",
    "href": "episode_1.html#what-are-the-main-challenges-or-decision-points",
    "title": "The RNA-seq workflow",
    "section": "What are the main challenges or decision points?",
    "text": "What are the main challenges or decision points?\n\nCounting or quantifying\nThis is not an exhaustive overview of RNA-seq data, but as a reminder, the raw data we receive in the form of FASTA files is a series of reads from somewhere in the genome. Each read is an indication, or measurement, of gene expression. We must take these reads, identify where in the genome they originated from and then make a measurement of how frequently we observed those reads. The more reads that are mapped back to a specific location in the genome, the more highly expressed that region (gene) is.\nThere are two major approaches to measure gene expression from reads: in the first method reads are aligned to a reference genome and are then ‘counted’, while in the second method reads are ‘quantified’ without the need to align to a reference (these are sometimes referred to as ‘pseudocounts’). There are arguments for each of these types, as well as other approaches, but a full discussion is beyond the scope of this workshop. We will point you towards some examples for each approach.\n\n\nLibrary sizes and composition\nDue to myriad biological and technical effects, each sample will have differing numbers of total reads (different library sizes). When we attempt to compare gene expression across samples, these different library sizes can cause genes to be over- or under-expressed. Before we can compare samples we must correct for library size differences.\nAdditionally, we need to be aware of library composition. If a library has one (or a few) genes that are very highly expressed, they can be thought of as ‘taking up’ a high proportion of the total library of reads. If we compare this skewed library with another library, some genes may look down-regulated in the skewed library.\n\n\nThe problem of heteroskedasticity\nHeteroskedasticity describes a situation where variance changes with the mean. In RNA-seq data as the mean expression level of a gene increases so too does the variance. This is an issue since some statistical approaches (specifically, the Poisson distribution) assume homoskedasticity (that is, that variance is not associated with the mean).\nTwo approaches to dealing with heteroskedasticity are common in the literature: transform the data to reduce (or remove) the mean-variance association or use a method that is robust to the mean-variance association (DESeq2 with the Negative Binomial distribution).\n\n\nGene length\nDoes gene length matter? You might be aware that if a gene is longer, it is more likely to have reads mapped to it. This is important to be aware of, but when we are identifying differentially expressed genes we are always comparing genes to themselves. Therefore, gene length is not a factor when identifying differentially expressed genes.\nHowever, gene length does need to be taken into account when we carry out over-representation analysis!",
    "crumbs": [
      "Day One"
    ]
  },
  {
    "objectID": "episode_1.html#introduction-to-the-dataset",
    "href": "episode_1.html#introduction-to-the-dataset",
    "title": "The RNA-seq workflow",
    "section": "Introduction to the dataset",
    "text": "Introduction to the dataset\nThe example data in this workshop are from Lee et al., and include a set of six Saccharomyces cerevisiae samples in two groups: three wild-type samples and three RNA-degradation mutants. Note that these particular mutants are signifcantly different from the wildtype and we expect large differences between our two sample groups.\nThe data is a subset of an RNA-sequencing experiment which used single end sequencing.\n\n Previous Page   Next Page",
    "crumbs": [
      "Day One"
    ]
  },
  {
    "objectID": "episode_4.html",
    "href": "episode_4.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "Exploratory analysis is a new section in this workflow, and will cover basic visualisations and characterisations of the data, then any initial transformations (e.g., library size) required before DGE.",
    "crumbs": [
      "Day One",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "episode_4.html#what-is-exploratory-analysis-and-why-is-it-necessary",
    "href": "episode_4.html#what-is-exploratory-analysis-and-why-is-it-necessary",
    "title": "Exploratory Analysis",
    "section": "What is exploratory analysis and why is it necessary?",
    "text": "What is exploratory analysis and why is it necessary?\nExploratory analysis is looking at the data. You are attempting to get a sense of what the data looks like, to notice outliers or issues (e.g., hey isn’t it weird that one of my wt samples groups with all the mutant samples, and one of the mut samples groups with the wt samples? I wonder what’s happening there?).\nIf your data set was a physical problem (like a squeaky wheel or a well wrapped present) you’d physically explore it (pick it up, is it heavy, do I shake it?). Exploratory analysis is the data science equivalent.",
    "crumbs": [
      "Day One",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "episode_4.html#exploratory-analysis",
    "href": "episode_4.html#exploratory-analysis",
    "title": "Exploratory Analysis",
    "section": "Exploratory analysis",
    "text": "Exploratory analysis\nUse the import data set or read.delim() function to import the file yeast_counts_all_chr, and name the object fcData. Also, load the dplyr library. We will look at some of the basic stats and features of our data object with View(), dim(), and names()\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n# If you are completing this workshop on the NeSI OpenOnDemand platform, run the code below to load the file. \n# fcData &lt;- read.delim(\"~/RNA_seq/yeast_counts_all_chr.txt\", comment.char=\"#\")\n\n## I'm loading from my own local machine, so my filepath is different:\nfcData &lt;- read.delim(\"~/Library/CloudStorage/OneDrive-UniversityofOtago/Documents/Training Coordinator 2022/GitHub/RNA-seq-data-analysis-workflow/data/yeast_counts_all_chr.txt\", comment.char=\"#\")\n\nView(fcData)\nfcData %&gt;% head()\n\n     Geneid Chr Start   End Strand Length ...STAR.SRR014335.Aligned.out.sam\n1   YDL248W  IV  1802  2953      +   1152                                52\n2 YDL247W-A  IV  3762  3836      +     75                                 0\n3   YDL247W  IV  5985  7814      +   1830                                 2\n4   YDL246C  IV  8683  9756      -   1074                                 0\n5   YDL245C  IV 11657 13360      -   1704                                 0\n6   YDL244W  IV 16204 17226      +   1023                                 6\n  ...STAR.SRR014336.Aligned.out.sam ...STAR.SRR014337.Aligned.out.sam\n1                                46                                36\n2                                 0                                 0\n3                                 4                                 2\n4                                 0                                 1\n5                                 3                                 0\n6                                 6                                 5\n  ...STAR.SRR014339.Aligned.out.sam ...STAR.SRR014340.Aligned.out.sam\n1                                65                                70\n2                                 0                                 1\n3                                 6                                 8\n4                                 1                                 2\n5                                 5                                 7\n6                                20                                30\n  ...STAR.SRR014341.Aligned.out.sam\n1                                78\n2                                 0\n3                                 5\n4                                 0\n5                                 4\n6                                19\n\nnames(fcData)\n\n [1] \"Geneid\"                            \"Chr\"                              \n [3] \"Start\"                             \"End\"                              \n [5] \"Strand\"                            \"Length\"                           \n [7] \"...STAR.SRR014335.Aligned.out.sam\" \"...STAR.SRR014336.Aligned.out.sam\"\n [9] \"...STAR.SRR014337.Aligned.out.sam\" \"...STAR.SRR014339.Aligned.out.sam\"\n[11] \"...STAR.SRR014340.Aligned.out.sam\" \"...STAR.SRR014341.Aligned.out.sam\"\n\n\n\n# [1] \"Geneid\"                           \n# [2] \"Chr\"                              \n# [3] \"Start\"                            \n# [4] \"End\"                              \n# [5] \"Strand\"                           \n# [6] \"Length\"                           \n# [7] \"...STAR.SRR014335.Aligned.out.sam\"\n# [8] \"...STAR.SRR014336.Aligned.out.sam\"\n# [9] \"...STAR.SRR014337.Aligned.out.sam\"\n# [10] \"...STAR.SRR014339.Aligned.out.sam\"\n# [11] \"...STAR.SRR014340.Aligned.out.sam\"\n# [12] \"...STAR.SRR014341.Aligned.out.sam\"\n\nTo help with our future data analysis, let’s rename the column names for the samples.\n\nnames(fcData)[7:12] &lt;- c(\"WT1\", \"WT2\", \"WT3\", \"MT1\", \"MT2\", \"MT3\")\n\nfcData %&gt;% head()\n\n     Geneid Chr Start   End Strand Length WT1 WT2 WT3 MT1 MT2 MT3\n1   YDL248W  IV  1802  2953      +   1152  52  46  36  65  70  78\n2 YDL247W-A  IV  3762  3836      +     75   0   0   0   0   1   0\n3   YDL247W  IV  5985  7814      +   1830   2   4   2   6   8   5\n4   YDL246C  IV  8683  9756      -   1074   0   0   1   1   2   0\n5   YDL245C  IV 11657 13360      -   1704   0   3   0   5   7   4\n6   YDL244W  IV 16204 17226      +   1023   6   6   5  20  30  19\n\n\nThat already looks much better. Now, let’s split out the count data, removing the annotation (e.g., Chr, Start, End) columns and adding Geneid back into the object as the row names. Having the geneid as rownames is much easier than having that data as a column. Save a copy of this counts object for loading later.\n\ncounts = fcData[,7:12]\nrownames(counts) &lt;- fcData$Geneid\ncounts %&gt;% head()\n\n          WT1 WT2 WT3 MT1 MT2 MT3\nYDL248W    52  46  36  65  70  78\nYDL247W-A   0   0   0   0   1   0\nYDL247W     2   4   2   6   8   5\nYDL246C     0   0   1   1   2   0\nYDL245C     0   3   0   5   7   4\nYDL244W     6   6   5  20  30  19\n\nsave(counts, file = \"counts.RData\")\n\nThis is a very basic, very ‘clean’ R object. We can see our sample names, gene names and counts easily. It is entirely possible to use this object for the rest of our analysis - and that’s what we will do today, for the sake of simplicity. This object is easy to interact with and is easy to mentally visualise and conceptualise.\nHowever, it’s worth noting that many newer R packages and workflows will require (or at least strongly encourage) you to use a “summarised experiment object”. If a vector is a one-dimensional data storage object, and a matrix is a two-dimensional data storage object, you can think of a summarised experiment object as a three-dimensional way of storing data. In a summarised experiment object where you have the genes on the y axis and the samples on the x axis with counts as the observations, the z axis could be a second matrix storing different observations. Summarised experiment objects can stack multiple data types in this way and build up a complex (but tidy) array of data. Summarised experiment objects are powerful but can be harder to mentally conceptualise and require more complex code to interact with the data they store.\nSummarised experiment objects are covered in the Carpentries Introduction to Data Analysis with R workshop.\n\n\n\nThe summarised experiment object, image from the Carpentries Incubator Introduction to Data Analysis with R workshop.",
    "crumbs": [
      "Day One",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "episode_4.html#remove-non-expressed-genes-optional",
    "href": "episode_4.html#remove-non-expressed-genes-optional",
    "title": "Exploratory Analysis",
    "section": "Remove non-expressed genes (optional)",
    "text": "Remove non-expressed genes (optional)\nIf you are working in a complex organism, many genes will not be expressed in all tissue types. It can be worth checking to see how many genes have zero (or e.g., &lt; 5) counts and opting to remove these from your data object.\nHow many genes have counts of zero? How many genes have fewer than e.g., 5 counts?\n\ncolSums(counts == 0)\n\nWT1 WT2 WT3 MT1 MT2 MT3 \n562 563 573 437 425 435 \n\n\n\n# WT1 WT2 WT3 MT1 MT2 MT3 \n# 562 563 573 437 425 435 \n\nBecause our zeros are not a massive part of the data (i.e., here they are &lt; 10% of the data), we will not remove them. However, they are worth noting because later we will consider logging our data, and zeros will cause an issue.",
    "crumbs": [
      "Day One",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "episode_4.html#visualising-our-data-set",
    "href": "episode_4.html#visualising-our-data-set",
    "title": "Exploratory Analysis",
    "section": "Visualising our data set",
    "text": "Visualising our data set\n\nboxplot(as.matrix(counts) ~ col(counts), \n        ylab = \"Counts\", xlab = \"Samples\", names = colnames(counts))\n\n\n\n\n\n\n\n\n This figure indicates we should consider logging our data (for the purpose of visualisations).\nLog transform the data, adding 0.5 to avoid log(0) errors.\n\nlogCounts = log2(as.matrix(counts) + 0.5)\n\nboxplot(as.matrix(logCounts) ~ col(counts), \n        ylab = \"Counts\", xlab = \"Samples\", names = colnames(counts), main = \"Log counts of samples\")\n\n\n\n\n\n\n\n\n\n\n\nLog counts are easier to visualise\n\n\nThe log count boxplots reveal a difference between our wt and mutant samples. As a reminder, these two strains of yeast are very different and we would not normally expect this level of difference (although that might vary depending on your experiment).\nWe can use density plots as another way to visualise differences between our two sample groups. First, create an object called lineColour which will colour our wt samples blue and our mutant samples red. Then create a density plot for the first sample, and use a loop to density lines for the remaining samples onto the plot.\n\nlineColour = c(\"blue\", \"blue\", \"blue\", \"red\", \"red\", \"red\")\nlineColour\n\n[1] \"blue\" \"blue\" \"blue\" \"red\"  \"red\"  \"red\" \n\nplot(density(logCounts[,1]), ylim=c(0,0.3), col=lineColour[1], xlab = \"log Counts\", main = \"Density plot\")\nfor(i in 2:ncol(logCounts)) lines(density(logCounts[,i]), col=lineColour[i])\n\n\n\n\n\n\n\n\n\n\n\nDensity plot\n\n\nA common question to ask at this point is, are the differences we can see between our two sample groups a true biological effect, or is this an experimental artifact? Often, we will not know the answer to this question. For now, it is enough to be aware of these differences and to keep them in mind moving forward.",
    "crumbs": [
      "Day One",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "episode_4.html#library-size",
    "href": "episode_4.html#library-size",
    "title": "Exploratory Analysis",
    "section": "Library size",
    "text": "Library size\nLibrary size refers to the total number of reads for the entire sample. If two samples have vastly different library sizes, genes would look differentially expressed between samples e.g., if one library was twice as large as another, all genes would look to be upregulated in the larger library sample. Before we compare counts across samples we need to check whether the library sizes are approximately similar, and if they are not, make a correction to account for the size differences.\n\ncolSums(counts) %&gt;% barplot(., ylab = \"Reads mapped per sample\", xlab = \"Sample\", main = \"Library size\")\n\n\n\n\n\n\n\n\n\n\n\nLibrary size are approximately similar across samples\n\n\nLibrary sizes are approximately equal across the samples. They are slightly lower in the mutants compared to the wt, but the difference is low (i.e., mean ~ 4.6 million and 4.8 million reads in mutant and wt respectively).\nIf we needed to correct for library size, we would use a two-step method. In the first step we calculate a sample-specific value called a size factor. If we take the raw read counts and divide by the size factor for each sample, the resulting values would be similar for all samples. The way in which size factors are implemented will differ depending on the analysis method used. For some methods the size factor must be manually used to correct for library sizes. For other methods (e.g., DESeq2, which we will use shortly) automatically estimates size factors and implements them into the differential gene expression analysis workflow.",
    "crumbs": [
      "Day One",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "episode_4.html#heteroskedasticity",
    "href": "episode_4.html#heteroskedasticity",
    "title": "Exploratory Analysis",
    "section": "Heteroskedasticity",
    "text": "Heteroskedasticity\nHeteroskedasticity is “heterogeneity of variance”. If the variance we observe is non-uniform across a range of values, we can describe the dataset as heteroskedastic. In RNA-seq, this means that variance depends on the mean expression of the gene. Wikipedia has a useful example:\n“A classic example of heteroscedasticity is that of income versus expenditure on meals. A wealthy person may eat inexpensive food sometimes and expensive food at other times. A poor person will almost always eat inexpensive food. Therefore, people with higher incomes exhibit greater variability in expenditures on food.”\n\nHeteroskedasticity\n\nExercise: Given the above example, do you expect genes with higher expression to have higher or lower variance than genes with low expression? That is, which will have higher variance - genes with high average expression or genes with low average expression?\nThis can be a surprisingly complex question to answer, and depends on how we think about variance. If a gene has a high mean expression (e.g., say 1000), then we can expect that the standard deviation will be a reasonably high number (e.g., let’s say 20). If a gene has low mean expression (e.g., 10), then the standard deviation will be small (e.g., 2). So, it is true to say that for genes with higher average expression, the standard deviation is greater.\nHowever, we also need to think about magnitude of variance, which we can do using the numbers from our above imaginary examples. For the gene with a mean expression of 1000 and sd of 20, going from 1000 to 1020 is not a big change. For the gene with the low mean expression, going from 10 to 12 is a large change.\nThis write-up gives an in-depth explanation of heteroskedasticity and how we can think about it.\nWe will look more at heteroskedasticity when we begin to identify differentially expressed genes, but for now it is important to be aware of the concept.\n\n Previous Page   Next Page",
    "crumbs": [
      "Day One",
      "Exploratory Analysis"
    ]
  },
  {
    "objectID": "episode_6.html",
    "href": "episode_6.html",
    "title": "Functional analysis",
    "section": "",
    "text": "Functional analysis is the process of taking a list of genes which we cannot meaningfully interact with and distilling information that we as researchers can then interpret. Generally, this involves grouping genes based on functions or processes, and determining which processes or functions appear to be different between your experimental groups.\nThere are different approaches to functional analysis e.g.,\nToday we will cover Over-representation analysis and functional class scoring. Links to pathway topology workflows are provided.",
    "crumbs": [
      "Day Two",
      "Functional analysis"
    ]
  },
  {
    "objectID": "episode_6.html#annotation",
    "href": "episode_6.html#annotation",
    "title": "Functional analysis",
    "section": "Annotation",
    "text": "Annotation\nThe basis of this stage of the analysis is to take individual genes, associate them with a function, and then to identify whether there are common themes of function in genes which have undergone changes in gene expression.\nThis first requires that our genes are annotated - we must be able to associate some type of function or process with a given gene. There are now many databases that maintain information and annotation on genes, including proposed and confirmed functions, pathways, processes etc.,. Annotation and databases is an entire area in itself, and not something we can cover today. For now, we will retrieve annotation information from different sources and attach the annotation to genes.",
    "crumbs": [
      "Day Two",
      "Functional analysis"
    ]
  },
  {
    "objectID": "episode_6.html#over-representation-analysis---the-hypergeometric-distribution",
    "href": "episode_6.html#over-representation-analysis---the-hypergeometric-distribution",
    "title": "Functional analysis",
    "section": "Over-representation Analysis - the hypergeometric distribution",
    "text": "Over-representation Analysis - the hypergeometric distribution\nLet’s say you have a list of 100 differentially expressed genes and, after annotation, you notice that 10 of these genes are involved in apoptosis. Is this a significant finding? Initially, you might think that yes it is significant, since 10% of your genes of interest share a role. However, to understand whether this is significant we need to consider the number of apoptosis-related genes in the genome. If, for example, we find that 12% of all genes in the genome are annotated as apoptosis-related, then seeing 10% of your differentially expressed genes with this annotation isn’t significant - it’s approximately what you would have expected. However, if you’d found 25% (or 0%!) THEN you might have something significant.\nOver-representation analysis is the use of a hypergeometric distribution to determine whether or not a sample group is undergoing coordinated gene expression changes. Statistically, it is asking what is the probability of getting 10 apoptosis genes in my list of 100 differentially expressed genes, given the number of apoptosis genes in the whole genome. We can use the Fisher’s Exact test and the hypergeometric distribution to ask whether being involved in apoptosis is independent of being significantly differentially expressed.\nWhat does this look like in practice? We can use a 2 x 2 table, where the four data points are (from top left to bottom right): the number of genes that are both in the category and are differentially expressed, the number of differentially expressed genes not in the category, the number of category genes that are not differentially expressed, and the number of genes that are neither in the category nor differentially expressed.\nLet’s attach some real numbers to those categories and visualise them. We have 10 apoptosis genes in our 100 differentially expressed genes, there are 500 genes annotated as apoptosis, and a total of 10,000 genes in the genome.\n\nhypergeoDistMatrix &lt;- matrix(c(10,490,90,9410),2,2)\nhypergeoDistMatrix\n\n     [,1] [,2]\n[1,]   10   90\n[2,]  490 9410\n\n\nQuickly, where did these numbers come from?\nTop left: 10 differentially expressed genes which are apoptosis related.\nTop right: 90 genes that are differentially expressed but are not apoptosis related.\nBottom left: 490 genes that are apoptosis related but not differentially expressed.\nBottom right: 9410 genes that are neither differentially expressed or apoptosis related.\nRow 1 sums to 100 (differentially expressed genes)\nRow 2 sums to 9,900 (non-differentially expressed genes)\nCol 1 sums to 500 (apoptosis genes)\nCol 2 sums to 9,500 (non-apoptosis genes)\nNow we can test for an association - or, more accurately, we can test the null hypothesis that being in the apoptosis category is independent of being in the differentially expressed category.\n\nfisher.test(hypergeoDistMatrix)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  hypergeoDistMatrix\np-value = 0.03328\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.9832142 4.1416491\nsample estimates:\nodds ratio \n  2.133664 \n\n\nThe p-value of 0.033 let’s us reject the null hypothesis and we conclude there is an association. In our hypothetical experiment, apoptosis-related genes are over-represented (enriched) in our differentially expressed gene list.\nWe don’t want to perform this test manually for every possible functional category (adjusting for multiple testing as we go), so we will look to existing software packages to do this for us. But first, let’s look at some caveats and considerations.\n\nCaveats\nThe hypergeometric distribution and Fisher’s test makes the assumption that all genes are equal. As an exercise, take 30 seconds to think about the ways that genes might not be equal, about RNA-sequencing, and what impact this could have on our test.\nOne major difference when looking across genes is gene length. RNA-seq tends to produce greater counts for longer genes, which is itself worth remembering, and this gives these genes a greater chance of being statistically differentially expressed. An additional problem is that some gene sets tend to be made up of primarily long genes (because certain processes or functions may require large, complex proteins). Because long genes have a greater chance of being differentially expressed, gene sets which are made up of long genes will have a greater chance of being enriched or over-represented.\nWe must carry out some type of correction for gene length when conducting over-representation analyses.\n\n\nGOseq\nWe will begin with a method called GOseq, which was developed as part of one of the publications initially identifying the gene length bias. An illustration from the full paper is below, demonstrating the relationship between differential gene expression and gene length and between differential expression and number of reads. \nThere is a strong positive correlation between differential gene expression and both total gene length and number of reads. Unless corrected for, gene length will bias your over-representation analysis.\nLoad the goseq package, the dplyr package (if you haven’t already), and use the supportedOrganisms() function to check whether your organism of interest is supported. The code for Saccharomyces cerevisiae is sacCer.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(goseq)\n\nLoading required package: BiasedUrn\n\n\nLoading required package: geneLenDataBase\n\n\n\n\nsupportedOrganisms() %&gt;% View()\n\nLoading required package: rtracklayer\n\n\nLoading required package: GenomicRanges\n\n\nLoading required package: stats4\n\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, table,\n    tapply, union, unique, unsplit, which.max, which.min\n\n\nLoading required package: S4Vectors\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following object is masked from 'package:geneLenDataBase':\n\n    unfactor\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, rename\n\n\nThe following object is masked from 'package:utils':\n\n    findMatches\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\n\nAttaching package: 'IRanges'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    collapse, desc, slice\n\n\nLoading required package: GenomeInfoDb\n\n\nWe need to create an object that will pass GOseq a list of all our genes, with the differentially expressed genes highlighted. To do this, we will use an ifelse() statement which will check every gene in our experiment and ask test if it has an adjusted p-value of less than 0.05. If adj-p-value is &lt; 0.05, return a “1” and if greater than (or equal to) 0.05, return a “0”. After that, add the gene names to this vector of 1s and 0s.\nNote #1: at the end of the previous exercise we also applied a logFC threshold for defining differentially expressed. We won’t use that here and will just use adjusted p-values for simplicity.\nNote #2: the tt object was created when we used limma to identify differentially expressed genes. tt is 7,127 rows, and has genes stored as row names, includes the logFC, and adj.P.Val columns.\n\nload(\"tt.RData\")\n\ngenes &lt;- ifelse(tt$adj.P.Val &lt;= 0.05, 1, 0)\n\nnames(genes) &lt;- rownames(tt)\n\nhead(genes)\n\nYAL038W YOR161C YML128C YMR105C YHL021C YDR516C \n      1       1       1       1       1       1 \n\ntail(genes)\n\nYOR122C YJR139C YEL046C YDL084W YOR326W YOL120C \n      0       0       0       0       0       0 \n\n\nNote #3: because tt is ordered (that is, all the genes with low p values are at the top), it’s worth using tail to check the bottom of the new genes object looks different to the top. Another useful check is to use the table() function to ask how many genes fall into each category.\n\ntable(genes)\n\ngenes\n   0    1 \n1987 5140 \n\n\nThis is a useful reminder about the conditions of this experiment: of the 7,127 genes in the tt object, 5,140 are differentially expressed. Most experiments will not have this level of differential expression.\nIF you did want to include the logFC cutoff, you can use the code below. This gives a more modest 1,891 differentially expressed genes. For the purposes of today’s workflow, we will continue with the larger gene list.\n\ngenes2 &lt;- ifelse(tt$adj.P.Val &lt;= 0.05 & (abs(tt$logFC) &gt; log2(2)), 1, 0)\nnames(genes2) &lt;- rownames(tt)\ntable(genes2)\n\ngenes2\n   0    1 \n5236 1891 \n\n\n\nMethodology\nWhat is GOSeq doing? How does it correct for gene length?\nRemember that we will still use the hypergeometric distribution and Fisher’s Exact test with the 2 x 2 table. Because we know that longer genes are more likely to in the differentially expressed category, we can think of this category as having more weight than it should. GOSeq will calculate a value for each gene which will offset this artificial weight. In other words, if our 2 x 2 table would have 10 genes in the top left square, and some of those genes are very long, GOSeq will treat the number as something slightly less than 10. By treating the value as less than 10, we have taken into account the fact that there shouldn’t really have been 10 genes in there in the first place if not for gene length bias.\nCalculate the weighting that should be assigned to each gene with the Probability Weighting Function (the nullp() function). Specify the object “gene” which is our list of all genes and whether they are differentially expressed or not, as well as the genome (“sacCer1” for our yeast genome), and the gene ID type. This will create a plot in which genes are placed into “bins” based on length, and then gene length vs proportion of differentially expressed is plotted. We can then inspect the pwf object.\n\npwf = nullp(genes, \"sacCer1\", \"ensGene\")\n\nLoading sacCer1 length data...\n\n\nWarning in pcls(G): initial point very close to some inequality constraints\n\n\n\n\n\n\n\n\npwf %&gt;% head()\n\n        DEgenes bias.data       pwf\nYAL038W       1      1504 0.8205505\nYOR161C       1      1621 0.8205505\nYML128C       1      1543 0.8205505\nYMR105C       1      1711 0.8205505\nYHL021C       1      1399 0.8205505\nYDR516C       1      1504 0.8205505\n\npwf %&gt;% tail()\n\n        DEgenes bias.data       pwf\nYOR122C       0       383 0.5916345\nYJR139C       0      1081 0.8170373\nYEL046C       0      1165 0.8196654\nYDL084W       0      1342 0.8205505\nYOR326W       0      4726 0.8205505\nYOL120C       0       563 0.6975162\n\n\nHere we can see that genes have been given a different pwf value based on the bias.data column. The pwf value will be less than 1, and indicates how much a gene should be ‘counted for’ if it is in the differentially expressed category in the 2 x 2 table.\nWe can predict that pwf and bias (length) are opposing values, and we can visualise that data:\n\npar(mfrow=c(1,2))\nhist(pwf$bias.data,30)\nhist(pwf$pwf,30)\n\n\n\n\n\n\n\n\nWe have a large number of genes which have low/no bias, and a correspondingly high number of genes with the max pwf weighting. A smaller number of genes have a greater bias and are given lower weighting.\nWe can now carry out our Fishers Exact test using the pwf value instead of the raw counts. We will use the goseq() function to perform this test, and output the over-representation data into an object.\n\n# BiocManager::install(\"org.Sc.sgd.db\")\n# BiocManager::install(\"AnnotationDbi\")\nlibrary(org.Sc.sgd.db)\n\nLoading required package: AnnotationDbi\n\n\nLoading required package: Biobase\n\n\nWelcome to Bioconductor\n\n    Vignettes contain introductory material; view with\n    'browseVignettes()'. To cite Bioconductor, see\n    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\n\n\nAttaching package: 'AnnotationDbi'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\n\nGO.wall = goseq(pwf, \"sacCer1\", \"ensGene\")\n\nFetching GO annotations...\n\n\nFor 1325 genes, we could not find any categories. These genes will be excluded.\n\n\nTo force their use, please run with use_genes_without_cat=TRUE (see documentation).\n\n\nThis was the default behavior for version 1.15.1 and earlier.\n\n\nCalculating the p-values...\n\n\n'select()' returned 1:1 mapping between keys and columns\n\nGO.wall %&gt;% head()\n\n       category over_represented_pvalue under_represented_pvalue numDEInCat\n1239 GO:0005622            1.826445e-17                        1       4300\n4848 GO:0042254            5.046546e-12                        1        369\n3578 GO:0022613            2.239622e-11                        1        442\n7249 GO:0110165            4.401672e-11                        1       4467\n8128 GO:1990904            5.902761e-11                        1        469\n5039 GO:0043228            9.417045e-11                        1       1266\n     numInCat                                 term ontology\n1239     5183   intracellular anatomical structure       CC\n4848      400                  ribosome biogenesis       BP\n3578      486 ribonucleoprotein complex biogenesis       BP\n7249     5434           cellular anatomical entity       CC\n8128      525            ribonucleoprotein complex       CC\n5039     1464       non-membrane-bounded organelle       CC\n\n\nWe will want to filter for only those categories with an adjusted p-value &lt; 0.05, and save that information to a new object for easy browsing.\nYou might have noticed that the GO.wall object doesn’t automatically perform an adjustment for multiple testing, so we will use the p.adjust() function to generate corrected p-values.\nWe will then use the adjusted p-values as a filtering criteria, and include the columns that correspond to category, term, and ontology (using the colnames() function, we can see that these are columns 1, 6 and 7).\n\nGO.wall.padj = p.adjust(GO.wall$over_represented_pvalue, method=\"fdr\")\n\nsum(GO.wall.padj &lt; 0.05)\n\n[1] 49\n\nGO.wall.sig = GO.wall[GO.wall.padj &lt; 0.05, c(1,6,7)]\n\nGO.wall.sig %&gt;% dim()\n\n[1] 49  3\n\nGO.wall.sig %&gt;% head()\n\n       category                                 term ontology\n1239 GO:0005622   intracellular anatomical structure       CC\n4848 GO:0042254                  ribosome biogenesis       BP\n3578 GO:0022613 ribonucleoprotein complex biogenesis       BP\n7249 GO:0110165           cellular anatomical entity       CC\n8128 GO:1990904            ribonucleoprotein complex       CC\n5039 GO:0043228       non-membrane-bounded organelle       CC\n\n\nAn optional filtering step can be applied here, which is to remove categories that have a large number of genes. Categories which have a large number of genes tend to be very broad terms, which are not very informative e.g., the categories “organelle”, “biological_process”, and “protein localization”.\nIf you choose to apply this filter, re-make the GO.wall.sig object. Here we are filtering with a requirement that the category contains fewer than 500 genes.\n\nGO.wall.sig &lt;- GO.wall[GO.wall.padj &lt; 0.05 & GO.wall$numInCat &lt; 500, c(1,6,7)]\n\nGO.wall.sig %&gt;% dim()\n\n[1] 20  3\n\nGO.wall.sig %&gt;% head()\n\n       category                                 term ontology\n4848 GO:0042254                  ribosome biogenesis       BP\n3578 GO:0022613 ribonucleoprotein complex biogenesis       BP\n3742 GO:0030684                          preribosome       CC\n1600 GO:0006364                      rRNA processing       BP\n4526 GO:0034470                     ncRNA processing       BP\n2966 GO:0016072               rRNA metabolic process       BP\n\n\nThis is a very useful way of getting a broad overview of what processes our differentially expressed genes are involved in. The final step we will take here is to use the GO.db package to retrieve more detailed information about the categories we have identified. GO.db will use the unique identifier in the category column and provide more information.\n\nlibrary(GO.db)\n\nGOTERM[[GO.wall.sig$category[1]]]\n\nGOID: GO:0042254\nTerm: ribosome biogenesis\nOntology: BP\nDefinition: A cellular process that results in the biosynthesis of\n    constituent macromolecules, assembly, and arrangement of\n    constituent parts of ribosome subunits; includes transport to the\n    sites of protein synthesis.\nSynonym: GO:0007046\nSynonym: ribosome biogenesis and assembly\nSecondary: GO:0007046\n\n\n\n\n\nClosing thoughts on GOseq\n\n Previous Page   Next Page",
    "crumbs": [
      "Day Two",
      "Functional analysis"
    ]
  },
  {
    "objectID": "episode_5.html",
    "href": "episode_5.html",
    "title": "Identify Differentially Expressed Genes",
    "section": "",
    "text": "Are those two groups significantly different from one another is a challenging question to answer. We can start by specifically asking our question as a statistical question: Are the differences we observe between the two groups greater than the differences we would expect to see by chance?",
    "crumbs": [
      "Day Two",
      "Identify Differentially Expressed Genes"
    ]
  },
  {
    "objectID": "episode_5.html#t-tests-and-p-values",
    "href": "episode_5.html#t-tests-and-p-values",
    "title": "Identify Differentially Expressed Genes",
    "section": "T-tests and p-values",
    "text": "T-tests and p-values\nThe statistical approach to this question is to begin with the null hypothesis (that there is no difference between the two groups) and test whether or not you can reject the null.\nTo test whether or not we can reject the null hypothesis we can calculate a test statistic:\n\n\n\nTest statistic formula\n\n\nHere we are taking the difference between the means of the two groups and then dividing that difference by some measure of variability - in this case, dividing by the standard error.\nIf the difference in means is LARGE relative to the variance, the test statistic will be large (indicating significance). If the difference between the means is small relative to the variance, the test statistic will be small, indicating the difference is probably not significant (i.e., the difference we observe is in-line with the variance we observe).\nOnce we have a test statistic we will calculate a p-value. The p-value is an indication of how likely we were to observe the given difference in means (or a more extreme difference) if there is truly no difference between the means. That is, how likely are we to see this difference due to random chance?\n\nInterpreting p-values\nHow do we interpret the p-value? We will specify a threshold (usually 0.05), and say that if a p-value is less than this threshold we will consider it a significant result. If the p-value is lower than the threshold we set, we will reject the null hypothesis (that the groups are identical) and accept the alternative (that there is a difference between the groups). The threshold we set is our level of “risk” that this event happened by chance alone.\nWhen we declare that a result with a p-value of less than 0.05 is significant, we are saying that we believe the difference to be true since, if there was truly no difference, such a result would happen less than 5% of the time.\nA useful mental analogy is to consider flipping a coin. We know that for a fair coin, the odds of getting heads is 50:50. Still, if we get four heads in a row it doesn’t worry us - it’s entirely plausible given the variation we expect. But if we get 50 heads in a row, while we know it’s statistically possible, we know it’s very unlikely to see such an extreme result. If we got to 100 heads in a row, we might instead start to question the fairness of the coin - we would reject the null hypothesis that the odds of heads and tails is identical.\n\n\nTypes of errors\nIt’s important to think about the two possible ways in which we could be wrong when testing a hypothesis like this: we could generate a false positive or a false negative.\n\nA false positive or Type I error is when we reject the null hypothesis when there is truly no significant difference.\nA false negative or Type II error is when we fail to reject the null hypothesis when there truly is a significant difference.\n\nWhen we select a p-value threshold of 0.05, we are accepting the fact that 5% of the time that the null hypothesis is true, we will reject it. This becomes hugely problematic when you are testing thousands of genes! In order to avoid a large number of false positives we must correct for multiple testing. The more tests we are doing, the more stringent we need to be. We will not cover multiple testing corrections in depth but will briefly mention two types:\n\nFamily-wise Error Rate (FWER), also called the Bonferroni and Holm corrections, is a highly stringent procedure. This approach will give the minimal possible number of false positives, but will miss some true positives. Good if false positives are particularly costly (e.g., if you are providing someone with a severe medical diagnosis).\nFalse Discovery Rate control (FDR), also called the Benjamini and Hochberg correction, is less conservative than FWER. This approach will identify more significant events, but expect a greater number of false positives. Use this approach if you are more concerned about missing something valuable and can afford a few false positives.\n\n\nExercise: Note down some key features of your experiment. Are you more inclined to use FWER or FDR? Which is more appropriate for your data and your experimental situation?\n\n\n\nModifying the t-test for RNA-seq\nMany biological experiments struggle with getting enough samples for statistical significance. In RNA-seq experiments it is common to see groups of three samples or replicates. This is especially problematic when using the t-test (or similar procedures that involve variance). When testing for differences in gene expression it is possible to encounter genes with a small difference in the mean between the two groups and, due to the small sample size, a very small level of variation (a small standard error). A small difference in the means divided by a very small standard error translates to a large test statistic, which is then translated to a small p-value and what looks like a highly significant result.\nSince this issue is caused by an artificially low standard error due to low sample numbers, a number of methods have proposed artificially increasing the standard error in some way. One way to implement this is through Shrinkage Estimation, which involves using Empirical Bayes methods to adjust individual test statistics based on the overall distribution of variances. During shrinkage estimation, small standard errors are made larger while large standard errors are made smaller.",
    "crumbs": [
      "Day Two",
      "Identify Differentially Expressed Genes"
    ]
  },
  {
    "objectID": "episode_5.html#identifying-differentially-expressed-genes-using-limma",
    "href": "episode_5.html#identifying-differentially-expressed-genes-using-limma",
    "title": "Identify Differentially Expressed Genes",
    "section": "Identifying differentially expressed genes using LIMMA",
    "text": "Identifying differentially expressed genes using LIMMA\nLimma involves data transformation and log scales to account for the data being in a non-normal distribution.\nLimma will create one of the special objects mentioned earlier in this workshop to store data. For Limma, this will be called a Digital Gene Expression object (DGE object). We will use the edgeR package to create this DGE object.\nLoad the libraries required for running limma, create a dge object where we specify counts (here you specify the matrix that contains your counts, in our case our object is already called counts). Then calculate normalisation factors, which will be used to adjust for library size differences, and calculate the log CPM. logCPM is, for each gene, how many counts there were per million reads. Because we are taking the log of this value, we will use the prior.counts=3 argument to add 3 to all read counts. This prevents any errors due to attempting to take the log of 0.\n\nlibrary(limma)\nlibrary(edgeR)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nload(\"counts.RData\")\n\ndge = DGEList(counts=counts)\ndge = calcNormFactors(dge)\nlogCPM = cpm(dge, log=TRUE, prior.count=3)\n\nhead(logCPM, 3)\n\n                 WT1        WT2        WT3        MT1        MT2        MT3\nYDL248W    3.7199528  3.5561232  3.2538405  3.6446399  3.7156488  3.9155366\nYDL247W-A -0.6765789 -0.6765789 -0.6765789 -0.6765789 -0.3140297 -0.6765789\nYDL247W    0.1484688  0.6727144  0.1645731  0.7843936  1.0395626  0.6349276\n\n\n\n#                  WT1        WT2        WT3        MT1        MT2        MT3\n# YDL248W    3.7199528  3.5561232  3.2538405  3.6446399  3.7156488  3.9155366\n# YDL247W-A -0.6765789 -0.6765789 -0.6765789 -0.6765789 -0.3140297 -0.6765789\n# YDL247W    0.1484688  0.6727144  0.1645731  0.7843936  1.0395626  0.6349276\n\n\nThe design matrix\nWe use the design matrix to specify our different groups. Initially we will just look at a simple design, where we will specify wt and mutant. However, you can also include other information e.g., if your two sample groups were comprised of male and females and you had reason to suspect there might be sex-dependent factors involved.\n\nconds = c(\"WT\",\"WT\",\"WT\",\"MT\",\"MT\",\"MT\")\n\ndesign = model.matrix(~conds)\n\ndesign\n\n  (Intercept) condsWT\n1           1       1\n2           1       1\n3           1       1\n4           1       0\n5           1       0\n6           1       0\nattr(,\"assign\")\n[1] 0 1\nattr(,\"contrasts\")\nattr(,\"contrasts\")$conds\n[1] \"contr.treatment\"\n\n\n\n\nAddressing heteroskedasticity with Limma\nLimma has a function called voom which we can use to address heteroskedasticity (a case where mean and variance are not independent). Voom will estimate the strength of the relationship between mean and variance and calculate “precision weights” for each gene. These are then used to normalise the data during the identification of differentially expressed genes.\nCreate an object, v, by calling the voom function on the Digital Gene Expression (dge) object. The v object will be used in later analyses. We will add information about our experimental setup by specifying the design object, and we will also call plot = TRUE to generate a plot showing the heteroskedasticity.\n\nv = voom(dge, design, plot = TRUE)\n\n\n\n\n\n\n\n\nThis plot highlights an earlier point about heteroskedasticity. For genes with low expression, the variance (here the square root of the standard deviation) is highly variable. Because the variance is dependent on the mean, we describe this data as heteroskedastic. Our v (voom) object contains information relating to the curve line (red) and makes an adjustment factor, so that the data will have a uniform relationship between the mean and variance.\nImportantly, other than changing the mean-variance relationship, voom does not cause significant changes to the underlying data.\n\nThe impacts of voom\n\nboxplot(v$E ~ col(v$E), ylab = \"Counts\", xlab = \"Samples\", main = \"Log counts after voom\")\n\n\n\n\n\n\n\n\n\n\n\nLog counts after voom\n\n\nWe can see that log counts in the voom object (that is, counts after voom has been applied to create a stable mean-variance relationship) have not drastically changed from when we plotted them earlier.\nWe can also visualise the data in the form of density plots:\n\nlineColour &lt;- ifelse(conds==\"MT\", \"red\", \"blue\")\nlineColour\n\n[1] \"blue\" \"blue\" \"blue\" \"red\"  \"red\"  \"red\" \n\nplot(density(v$E[,1]), ylim=c(0,0.3), col=lineColour[1])\nfor(i in 2:ncol(logCPM)) lines(density(v$E[,i]), col=lineColour[i])\n\n\n\n\n\n\n\n\n\n\n\nCount density plots after voom\n\n\nWe can see that voom has not removed the differences between our sample groups.\nWhat if we wanted to remove these differences? Remember, we (probably) cannot be certain as to whether these differences are biological or technical. In cases like this it can be good to run your analysis multiple ways - first without removing the differences, and then after removing the differences. If a decision like this causes your results to change drastically then you need to be aware of the impact your choices are having so that you can make an informed decision about how to investigate further.\nWe have included a supplementary page on quantile normalisation, which is one method you can use to remove differences between sample groups. Note that quantile normalisation is a drastic intervention and should not be undertaken without a real need.\n\n\n\nDetecting differentially expressed genes\nNow that we have reached this point in the analysis, identifying differentially expressed genes is relatively simple in terms of the code we run. We will use the lmFit() command to fit a linear model for every gene using our v (voom) object and the design matrix, then use the eBayes() function on the linear model to perform Empirical Bayes shrinkage estimation and return moderated test statistics.\n\nfit = lmFit(v, design)\n\nfit = (eBayes(fit))\n\nThe fit object is complex to look at. We will use the topTable function to retrieve the useful information (gene name, logFC, adjusted p values), and then filter for significant genes only. We will also save the topTable object (tt) for use in the next episode.\n\ntt = topTable(fit, coef=ncol(design), n=nrow(counts))\n\nhead(tt)\n\n           logFC  AveExpr        t      P.Value    adj.P.Val        B\nYAL038W 2.313985 10.80214 319.5950 3.725452e-13 8.850433e-10 21.08951\nYOR161C 2.568389 10.80811 321.9628 3.574510e-13 8.850433e-10 21.08187\nYML128C 1.640664 11.40819 286.6167 6.857932e-13 9.775297e-10 20.84520\nYMR105C 2.772539  9.65092 331.8249 3.018547e-13 8.850433e-10 20.16815\nYHL021C 2.034496 10.17510 269.4034 9.702963e-13 1.152550e-09 20.07857\nYDR516C 2.085424 10.05426 260.8061 1.163655e-12 1.184767e-09 19.87217\n\nsum(tt$adj.P.Val &lt; 0.05)\n\n[1] 5140\n\n# 5140 genes with an adjusted p-value less than 0.05. \n\nsum(tt$adj.P.Val &lt; 0.01)\n\n[1] 4566\n\n# 4566 genes with an adjusted p-value less than 0.01.\n\nsave(tt, file=\"tt.RData\")\n\n\nExercise\nOur output includes an adjusted p-value. Use the help function (type a ? in front of any function name) to learn what method was used to adjust for multiple testing. Using information from the help menu, change the correction method to something different. How does this impact your results?\n\n\n\nVolcano plots\nVolcano plots are a useful and commonly used method to visualise differentially expressed genes within your data set. We will produce the plot over multiple steps, but in practice you would jump straight to the final plot.\nIn the following code we will first produce the volcano plot itself, which shows the distribution of all genes according to p-value and log2 fold change. We will then add a horizontal line indicating the threshold of a p-value &lt; 0.05.\n\nvolcanoplot(fit, coef=2) \n\nabline(h=-log10(0.05))\n\n\n\n\n\n\n\nsigGenesLimmaPval = which(tt$adj.P.Val &lt;= 0.05)\nlength(sigGenesLimmaPval)\n\n[1] 5140\n\nvolcanoplot(fit, coef=2)\npoints(tt$logFC[sigGenesLimmaPval], -log10(tt$P.Value[sigGenesLimmaPval]), col='red', pch=16, cex=0.5)\n\n\n\n\n\n\n\n\nStatistical significance is often not the only measure we will use to call genes differentially expressed. It is common to apply an additional threshold of gene expression needing to double or halve to be considered differentially expressed. On the log2FC scale, this is log2(2). Re-plot the volcano plot and when calling significant genes, apply both the adjusted p-value &lt; 0.05 and log2FC &gt; 1 thresholds.\n\nsigGenesLimma = which(tt$adj.P.Val &lt;= 0.05 & (abs(tt$logFC) &gt; log2(2)))\nlength(sigGenesLimma)\n\n[1] 1891\n\n# 1891\n\nvolcanoplot(fit, coef=2, ylab = \"P-value (-log10 scale)\", main = \"Significantly differentially expressed genes\")\npoints(tt$logFC[sigGenesLimma], -log10(tt$P.Value[sigGenesLimma]), col='red', pch=16, cex=0.5)\n\n\n\n\n\n\n\n\n\n\n\nSignificantly differentially expressed genes with Limma\n\n\nThis plot is an important “sanity check”. We can logically check that our significant genes (in red) are those with both a p-value &lt; 0.05 and a log2 fold change greater than 1 (which is equivalent to a doubling or halving of gene expression). If we observe red marked dots in the center or bottom of the plot, we could recognise an error has occurred.\n\n\nDifferentially expressed genes stored\nWe now have an object, sigGenesLimma, that shows which genes meet our criteria for differentially expressed. Due to a quirk of how we have made our objects, this object only contains the numbers of the rows with significant genes. Let’s update our object to include all the information for our list of significantly differentially expressed genes. We will return to this object later, but for now we will move on and identify differentially expressed genes using a different method.\n\nNote: it’s not normally a good idea to over-write an object as I’m doing here, but for the purposes of the workshop, it’s easiest to keep this object name.\n\n\nsigGenesLimma &lt;- tt[sigGenesLimma, ]",
    "crumbs": [
      "Day Two",
      "Identify Differentially Expressed Genes"
    ]
  },
  {
    "objectID": "episode_5.html#identifying-differentially-expressed-genes-using-deseq2",
    "href": "episode_5.html#identifying-differentially-expressed-genes-using-deseq2",
    "title": "Identify Differentially Expressed Genes",
    "section": "Identifying differentially expressed genes using DESeq2",
    "text": "Identifying differentially expressed genes using DESeq2\nDESeq2 is a highly-regarded R package for analysing RNA-seq data. DESeq2 uses a negative binomial method to model the count data, and combines this with a generalised linear model (GLM) to identify differentially expressed genes. From a statistical point of view, a negative binomial distribution is For more about the DESeq2 package, you can read the original article.\n\nThe DESeq2 object\nThe DESeq2 package requires a specific data storage object called a “DESeq Data Set” or DDS object. The DDS object contains not just the count data, but also the design matrix we created earlier (which is used to specify groups for comparison). This object can be created using a function from the DESeq2 package and requires that we specify a source of count data (as a matrix), column data (column names, mutant and wildtype), and a design matrix (the same one we used for Limma above). Once we have created the dds object, we can view the data stored within using the counts function.\nNote that if your data is stored in another format the DESeq2 function we will use to import data can be modified to work on other data formats e.g., a summarisedExperimentObject.\n\nlibrary(DESeq2)\n\nLoading required package: S4Vectors\n\n\nLoading required package: stats4\n\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\n\nThe following object is masked from 'package:limma':\n\n    plotMA\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, table,\n    tapply, union, unique, unsplit, which.max, which.min\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, rename\n\n\nThe following object is masked from 'package:utils':\n\n    findMatches\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\n\nAttaching package: 'IRanges'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    collapse, desc, slice\n\n\nLoading required package: GenomicRanges\n\n\nLoading required package: GenomeInfoDb\n\n\nLoading required package: SummarizedExperiment\n\n\nLoading required package: MatrixGenerics\n\n\nLoading required package: matrixStats\n\n\n\nAttaching package: 'matrixStats'\n\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\n\n\nAttaching package: 'MatrixGenerics'\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n    colWeightedMeans, colWeightedMedians, colWeightedSds,\n    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n    rowWeightedSds, rowWeightedVars\n\n\nLoading required package: Biobase\n\n\nWelcome to Bioconductor\n\n    Vignettes contain introductory material; view with\n    'browseVignettes()'. To cite Bioconductor, see\n    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\n\n\nAttaching package: 'Biobase'\n\n\nThe following object is masked from 'package:MatrixGenerics':\n\n    rowMedians\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    anyMissing, rowMedians\n\ndds = DESeqDataSetFromMatrix(countData = as.matrix(counts), \n                              colData = data.frame(conds=factor(conds)), \n                              design = formula(~conds))\n\ncounts(dds) %&gt;% head()\n\n          WT1 WT2 WT3 MT1 MT2 MT3\nYDL248W    52  46  36  65  70  78\nYDL247W-A   0   0   0   0   1   0\nYDL247W     2   4   2   6   8   5\nYDL246C     0   0   1   1   2   0\nYDL245C     0   3   0   5   7   4\nYDL244W     6   6   5  20  30  19\n\n\nNow fit the DESeq2 generalised linear model to the data.\n\ndds = DESeq(dds)\n\nestimating size factors\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\nNote the outputs from running this function. What are these outputs?\n\nEstimating size factors: assesses library size and calculates factors to adjust for differences between samples. This also adjusts for compositional differences (e.g., if gene X in sample 1 takes up a very large proportion of all available reads, other genes will have correspondingly fewer genes. If this effect is not uniform across samples, it can be corrected for during this stage).\nDispersion: adjusting for heteroskedasticity. DESeq2 makes use of variability estimates from not just one gene, but from all genes to make estimates about overall levels of variance. By bringing in (or “borrowing”) information from other genes, DESeq2 compensates for a small number of samples (which can lead to artificially small variance estimates otherwise).\nFitting model and testing: fitting the GLM and identifying differentially expressed genes.\n\nWe will now create a new object, res, to store the results in. We can access those results using either the head function or the summary function, which will give us slightly different information - both are valid and useful ways of familiarising yourself with the data.\n\nres = DESeq2::results(dds)\n\nres %&gt;% head()\n\nlog2 fold change (MLE): conds WT vs MT \nWald test p-value: conds WT vs MT \nDataFrame with 6 rows and 6 columns\n           baseMean log2FoldChange     lfcSE      stat     pvalue       padj\n          &lt;numeric&gt;      &lt;numeric&gt; &lt;numeric&gt; &lt;numeric&gt;  &lt;numeric&gt;  &lt;numeric&gt;\nYDL248W   56.223032      -0.233947  0.227842 -1.026796 0.30451655 0.36636393\nYDL247W-A  0.139771      -0.525577  4.080473 -0.128803 0.89751357 0.91739265\nYDL247W    4.242821      -0.810055  0.833254 -0.972158 0.33097181 0.39485370\nYDL246C    0.618241      -1.032636  2.156090 -0.478939 0.63198178 0.69151483\nYDL245C    2.848658      -1.978175  1.175885 -1.682287 0.09251326 0.12300284\nYDL244W   13.088335      -1.582335  0.512879 -3.085203 0.00203413 0.00328598\n\nres %&gt;% summary()\n\n\nout of 6830 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 2520, 37%\nLFC &lt; 0 (down)     : 2521, 37%\noutliers [1]       : 0, 0%\nlow counts [2]     : 0, 0%\n(mean count &lt; 0)\n[1] see 'cooksCutoff' argument of ?results\n[2] see 'independentFiltering' argument of ?results\n\n\nThe res object contains information for all genes tested. It is practical to create a new object that contains only the genes we consider differentially expressed based on the thresholds (p-value, logFC) and methods (e.g., multiple testing adjustment) that suit our situation.\nWe will remove any rows that have NAs in the results object, then pull out only those with an adjusted p-value less than 0.05.\n\nres &lt;- na.omit(res)\n\n# Keep all rows in the res object if the adjusted p-value &lt; 0.05\nresPadj &lt;- res[res$padj &lt;= 0.05 , ]\n\nresPadjLogFC &lt;- res[res$padj &lt;= 0.05 & abs(res$log2FoldChange) &gt; log2(2),]\n\n# Get dimensions\nresPadj %&gt;% dim()\n\n[1] 4811    6\n\nresPadjLogFC %&gt;% dim()\n\n[1] 1830    6\n\n# 1830 x 6\n\n\nExercise\nBioinformatics can often be something of a “black box” - we execute a function by feeding in some data, and get new data as output. It’s very easy to fall into the trap of believing your data without asking all the necessary questions. For example, we’ve just used an adjusted p-value for our threshold for significance. Did you question which method was used to adjust for multiple testing? See if you can identify, using the help menu as we’ve done above, which method was used here. Try and specify a different method for multiple testing corrections and note how this alters your output.\n\n\n\nDifferentially expressed gene lists\nTo recap, we’ve now produced two lists of differentially expressed genes - one produced with the Limma method (called sigGenesLimma), one with DESeq2 (called resPadjLogFC). In both cases we used the same threshold for statistical significance (adjusted p-value &lt; 0.05, using the FDR correction) and biological significance (logFC more than 1 or less than negative 1, which is equivalent to a doubling or halving gene expression). In our run-through, Limma identified 1,891 differentially expressed genes, while DESeq2 identified 1,830 genes.\nThese numbers are very similar, but what’s the actual overlap between these two groups? A venn diagram is an easy way to visualise the relationship between the two groups. First, load the gplots library which we will use for making venn diagrams. Create an object, which we will call setlist, that lists the two gene sets (in our case, this will be the rownames of the objects we created with Limma and DESeq2). Finally, use the venn() function to create a venn diagram.\n\nlibrary(gplots)\n\n\nAttaching package: 'gplots'\n\n\nThe following object is masked from 'package:IRanges':\n\n    space\n\n\nThe following object is masked from 'package:S4Vectors':\n\n    space\n\n\nThe following object is masked from 'package:stats':\n\n    lowess\n\nsetlist &lt;- list(Limma = rownames(sigGenesLimma), \n                DESeq2 = rownames(resPadjLogFC))\n\nvenn(setlist)\n\n\n\n\n\n\n\n\n\n\n\nVenn diagram of differentially expressed genes identified with Limma and DESeq2\n\n\nThis is very strong concordance between our two methods and gives us high confidence in our approach. At this point you could take the overlap (n = 1799 genes), take all genes (n = 1799 + 31 + 92), or take genes from just one method (more relevant if one method was conservative compared to the other). Exactly how you resolve this step will depend on the exact shape of your venn diagram and your context.\nFor simplicity, we will take the list of genes identified by limma. We will save the R object, and then see in the next episode how we can derive meaningful biological information from this long list of genes.\n\nsave(sigGenesLimma, file = 'topTable.RData')\n\n\n Previous Page   Next Page",
    "crumbs": [
      "Day Two",
      "Identify Differentially Expressed Genes"
    ]
  },
  {
    "objectID": "episode_2.html",
    "href": "episode_2.html",
    "title": "Sequence Quality - FastQC, MultiQC and Cutadapt",
    "section": "",
    "text": "In this episode we will look at quality assessment of the sequencing data. We will begin with FastQC, a free program which will analyse raw sequence data and output a visual summary. Then we will introduce MultiQC, a useful tool which can be used to combine results from different software packages into a single, coherent report. Finally we will look at read cleaning.",
    "crumbs": [
      "Day One",
      "Sequence Quality - FastQC, MultiQC and Cutadapt"
    ]
  },
  {
    "objectID": "episode_2.html#quality-assessment-with-fastqc",
    "href": "episode_2.html#quality-assessment-with-fastqc",
    "title": "Sequence Quality - FastQC, MultiQC and Cutadapt",
    "section": "Quality Assessment with FastQC",
    "text": "Quality Assessment with FastQC\nQuality assessment of high throughput sequencing data has been covered in depth and involves measurements of a number of sequence traits. FastQC is one of many options available for assessing the quality and is the one we will look at today. The output of FastQC is a visual representation of sequence quality which, if required, can then be used to investigate certain traits in more depth. FastQC provides a handy three colour binning system: green ticks for high quality, orange exclamation marks for middling quality that may require a manual investigation, and red crosses for low quality.\nIf a sample has low quality scores in one or more aspects, this does not necessarily mean we need to remove this sample from our analysis. The next steps of the workflow involve performing some sequence cleaning and trimming which may increase the overall quality of the sequence. However, it is still worth performing QC initially and comparing this to the post-cleaning sequence.\n\nGenerating FastQC reports\nIn a terminal window navigate to your RNA-seq directory. Ensure you have a sub-directory that contains your raw sequence files (name this Raw), and separate sub-directory in which to store quality control outputs (name this QC).\n# While in the RNA-seq directory containing the directories Raw and QC:\nfastqc -o QC/ Raw/*\nThe FastQC tool will generate reports on all files within the directory Raw, and output them into the QC directory. You will see an output similar to:\n    Started analysis of SRR014335-chr1.fastq\n    Approx 5% complete for SRR014335-chr1.fastq\n    Approx 10% complete for SRR014335-chr1.fastq\n    Approx 15% complete for SRR014335-chr1.fastq\n    Approx 20% complete for SRR014335-chr1.fastq\n    Approx 25% complete for SRR014335-chr1.fastq\n    Approx 30% complete for SRR014335-chr1.fastq\n    Approx 35% complete for SRR014335-chr1.fastq\nIf we now look within the QC directory, we should see two types of files for each of our six samples. These should be called “sampleName_fastqc.html” and “sampleName_fastqc.zip”.\n\n\nViewing the FastQC results\nThe html file outputs from FastQC can be opened in a browser for viewing. For each sample we have an overview of the quality for different metrics. On the left hand side of the report is a navigation bar that works as a broad summary. An ideal sample would have green tick marks for every measurement, while a terrible sample would have red crosses for every measurement. Most samples will fall somewhere in between and there is an element of interpretation.\n\n\n\nFastQC html summary\n\n\n\n\nFastQC text output\nFastQC outputs text files into a zipped directory. The text files contain detailed results of the various statistical tests done during QC. We might want to examine one or more of our QC reports in more detail, and here we will concatenate the QC reports from each of the six samples into a single text file called fastqc_summaries.txt.\nNavigate to the QC directory containing the sampleName_fastqc.zip outputs for each sample. Unzip the files and use the ls command to see what files are created for each sample, then use the less command to preview the summary.txt file for the first sample (SRR014335-chr1_fastqc).\nfor filename in *.zip\ndo\nunzip $filename\ndone\n\nls SRR014335-chr1_fastqc\n# Should contain the files:\n# fastqc_data.txt  fastqc.fo  fastqc_report.html  Icons/  Images/  summary.txt\n\n\nless SRR014335-chr1_fastqc/summary.txt\n# Use \"q\" to quit out of the window when you are done. \nCreate a single document that contains the information from summary.txt from all six samples, and name that document fastqc_summaries.txt\ncat */summary.txt &gt; ~/RNA_seq/QC/fastqc_summaries.txt\nOpen the fastqc_summaries.txt file and search for any of the samples that have failed the QC statistical tests.\nNeed to include an explanation of phred quality + 33",
    "crumbs": [
      "Day One",
      "Sequence Quality - FastQC, MultiQC and Cutadapt"
    ]
  },
  {
    "objectID": "episode_2.html#multiqc---multi-sample-analysis",
    "href": "episode_2.html#multiqc---multi-sample-analysis",
    "title": "Sequence Quality - FastQC, MultiQC and Cutadapt",
    "section": "MultiQC - multi-sample analysis",
    "text": "MultiQC - multi-sample analysis\nThe MultiQC application will create a report based on all documents in a given directory. MultiQC will take inputs from many different software applications, including fastQC. The report is a concise, clear document that can be used to track samples as they progress through various stages of the analysis.\nTo generate the MultiQC report first navigate to the RNA_seq directory and create a new output directory called MultiQC, then copy all target files to that directory (initially target files will be the FastQC documents generated above). Finally, execute the multiqc command.\ncd ~/RNA_seq/\n\nmkdir MultiQC\n\ncd MultiQC\n\ncp -r ../QC/* ./\n\nmultiqc .\n\nls -F\n\n# ls -F will show multiqc_data/ and multiqc_report.html\n\n\n\nMultiQC report\n\n\nAfter each step in the analysis (e.g., read cleaning, adaptor trimming) we will copy over new reports and summaries to the MultiQC directory and re-run the multiqc command. New information will be appended to the MultiQC report at each stage.",
    "crumbs": [
      "Day One",
      "Sequence Quality - FastQC, MultiQC and Cutadapt"
    ]
  },
  {
    "objectID": "episode_2.html#cleaning-reads",
    "href": "episode_2.html#cleaning-reads",
    "title": "Sequence Quality - FastQC, MultiQC and Cutadapt",
    "section": "Cleaning reads",
    "text": "Cleaning reads\nIn the previous section, we took a high-level look at the quality of each of our samples using FastQC. We visualized per-base quality graphs showing the distribution of read quality at each base across all reads in a sample and extracted information about which samples fail which quality checks. Some of our samples failed quite a few quality metrics used by FastQC. This doesn’t mean that our samples should be thrown out! It’s very common to have some quality metrics fail, and this may or may not be a problem for your downstream application.\nIn this section we will perform read cleaning using Cutadapt. Cutadapt will trim poor quality bases in a threshold-specific manner and will filter out poor quality reads. Cutadapt can be used to remove primers, poly-A tails, and adapter sequences (discussed below). See the full User guide to learn more about Cutadapt.\n\nCutadapt: Adapter trimming\nAdapters are short, known sequences that become embedded in your reads as part of the sequencing process. Before we work with our reads we want to remove these adaptors. Because adapters are manually added to the sequencing reaction, we should know exactly what these sequences are. In our example, we know that the adapter sequences are AACCGGTT. If you do not have access to information about what adapters were used in your sequence, some software can detect certain adaptors (e.g., Trimmomatic, which has a library of Illumina adapter sequences - these will be screened against reads and if a match is detected, those adapters will be trimmed).\nWe will perform adapter trimming simultaneously with quality trimming, done below.\n\n\nCutadapt: Quality trimming\nQuality trimming is the process of removing low-quality bases from the end of reads. Usually during sequencing it is the end (or start) of the read which has the lowest quality (Can I find an example image of this from our FastQC?). By trimming only the low-quality ends of the reads, we improve our overall sequence quality without sacrificing too much data.\nHere we will use Cutadapt and the -q flag to specify the lower threshold of quality we are willing to accept at the 3’ end of our reads. Two things to note are that a) we can apply the threshold cutoff to both the 5’ and the 3’ end, but for Illumina sequencing the 5’ end is usually high quality, and b) this requires our per-based quality scores to be encoded as phred quality + 33.\nTo begin, navigate to the RNA_seq directory and create a new directory called Trimmed which we will use to store our modified files. We will then use cutadapt and the “-q” flag to remove bases from the 3’ end of our reads if they have a phred quality score below 20. Simultaneously we will use the “-a” flag to remove our adaptors (supplied as AACCGGTT).\nCutadapt will produce a trimmed file as well as a summary in the form of a log file.\nAs a reminder, you should never modify your raw data (and should ideally have raw data backed up in a remote and secure location).\ncd ~/RNA_seq\n\nmkdir Trimmed\n\ncutadapt -q 20 -a AACCGGTT -o Trimmed/SRR014335-chr1_cutadapt.fastq Raw/SRR014335-chr1.fastq &gt; Trimmed/SRR014335-chr1.log\n\nless Trimmed/SRR014335-chr1.log\nNow trim all samples in the Raw directory:\ncd ~/RNA_seq/Raw\nfor filename in *.fastq\ndo \nbase=$(basename ${filename} .fastq)\ncutadapt -q 20 -a AACCGGTT -o ../Trimmed/${base}.trimmed.fastq ${filename} &gt; ../Trimmed/${base}.log\ndone\n\n\nCutadapt: Filtering reads\nCutadapt can also filter reads. That is, reads which meet (or more often, fail to meet) a specific criteria are either discarded or redirected to a separate output file.\nExamples of read filtering include using the -m flag to specify a minimum length. If reads are below a certain length, they will be discarded or redirected. This can be a useful way to remove reads that have a length of zero. The user guide for Cutadapt has many other ways in which you can filter reads.",
    "crumbs": [
      "Day One",
      "Sequence Quality - FastQC, MultiQC and Cutadapt"
    ]
  },
  {
    "objectID": "episode_2.html#multiqc-update",
    "href": "episode_2.html#multiqc-update",
    "title": "Sequence Quality - FastQC, MultiQC and Cutadapt",
    "section": "MultiQC update",
    "text": "MultiQC update\nWe can now copy the log files from Cutadapt into our MultiQC directory and re-run the multiqc command to generate an updated report which includes our read cleaning information.\nNavigate to the MultiQC directory, copy over the log files from the Trimmed directory and rerun the multiqc command.\ncd ../MultiQC\n\ncp ../Trimmed/*log .\n\nmultiqc .\n\n\n\nUpdated MultiQC report\n\n\nWe can now see the Cutadapt metrics have been added to the report.\n\n Previous Page   Next Page",
    "crumbs": [
      "Day One",
      "Sequence Quality - FastQC, MultiQC and Cutadapt"
    ]
  },
  {
    "objectID": "day_2_overview.html",
    "href": "day_2_overview.html",
    "title": "Day Two Overview",
    "section": "",
    "text": "Plan for day 2",
    "crumbs": [
      "Day Two"
    ]
  },
  {
    "objectID": "episode_6.html#section",
    "href": "episode_6.html#section",
    "title": "Functional analysis",
    "section": "",
    "text": "Previous Page   Next Page",
    "crumbs": [
      "Day Two",
      "Functional analysis"
    ]
  }
]